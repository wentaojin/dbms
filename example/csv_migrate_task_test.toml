task-name = "csv-task01"
datasource-name-s = "oracle151"
datasource-name-t = "tidb161"
comment = "测试数据源"

[case-field-rule]
# 控制配置文件内所有 *-s 参数配置值以及 column-route-rules 中的 key 值
# 1，参数值是 0 代表以当前配置文件为准，与源端对比
# 2，参数值是 1 代表统一转换值为小写，与源端对比
# 3，参数值是 2 代表统一转换值为大写，与源端对比
case-field-rule-s = "0"
# 控制配置文件 *-t 以及 column-route-rules 中的 value 值
# 1，参数值是 0 代表统一转换数据库名、表名、字段名值以当前配置文件为准
# 2，参数值是 1 代表统一转换数据库名、表名、字段名值为小写
# 3，参数值是 2 代表统一转换数据库名、表名、字段名值为大写
case-field-rule-t = "2"

[schema-route-rule]
schema-name-s = "SCOTT"
schema-name-t = "uccis"
include-table-s =  ["T8"]
#include-table-s = ["MARVIN00","MARVIN01","MARVIN05"]
#include-table-s = ["PM_TC_PROCESS_CODE","MARVIN00","MARVIN01"]
exclude-table-s = []

[[schema-route-rule.table-route-rules]]
table-name-s = ""
table-name-t = ""
column-route-rules = {}

[[data-migrate-rules]]
table-name-s = ""
# 基于数据切分策略，获取指定数据迁移表的查询范围
enable-chunk-strategy = false
# 指定数据迁移表的查询范围
# 注意自定义数据迁移表之后，对应表将只迁移该部分数据
where-range = ""
# 指定分片 chunk sql 查询 hint
sql-hint-s = ""

# 表串行运行，表内并发
[csv-migrate-param]
# 初始化表任务并发数
table-thread = 2
# 数据写入批量大小
batch-size = 500
# 数据校验写 meta 数据库并发数
write-thread = 4
# 数据表大小放大系数，当磁盘空间小于数据表大小时，则不执行导出任务自动跳过，直至满足条件或者任务结束为止
disk-usage-factor = "1.5"
# CSV 文件是否包含表头
header = true
# 字段分隔符，支持一个或多个字符，默认值为 ','
separator = '|#|'
# 行尾定界字符，支持一个或多个字符, 默认值 "\r\n" （回车+换行）
terminator = "|+|\r\n"
# 数据文件字符集
data-charset-t = "utf8mb4"
# 字符串引用定界符，支持一个或多个字符，设置为空表示字符串未加引号
delimiter = '"'
# 数据 NULL 空值表示，设置为空默认 NULL -> NULL
null-value = 'NULL'
# 使用反斜杠 (\) 来转义导出文件中的特殊字符
escape-backslash = true
# 任务 chunk 数，固定动作，一旦确认，不能更改，除非设置 enable-checkpoint = false，重新导出导入
# 1、代表每张表每并发处理多少行数
# 2、建议参数值是 insert-batch-size 整数倍，会根据 insert-batch-size 大小切分
chunk-size = 100000
# 数据文件输出目录, 所有表数据输出文件目录，需要磁盘空间充足，当数据目录不足某个表存储时将自动跳过数据导出直至满足条件的表或者任务结束
# 目录格式：/data/${target_dbname}/${table_name}
output-dir = "/users/marvin/gostore/dbms/data/csv"
# 表内 SQL 执行并发数，表示同时多少并发 SQL 读取上游表数据，可动态变更
sql-thread-s = 32
# 指定分片 chunk sql 查询 hint
sql-hint-s = "/*+ PARALLEL(8) */"
# calltimeout，单位：秒
call-timeout = 36000
# 关于全量断点恢复
#   - 若想断点恢复，设置 enable-checkpoint = true,首次一旦运行则 chunk-size 数不能调整，
#   - 若不想断点恢复或者重新调整 chunk-size 数，设置 enable-checkpoint = false,重新运行全量任务
#   - 无法断点续传期间，则需要设置 enable-checkpoint = false 重新导入导出
enable-checkpoint = true
# 是否一致性读 ORA
enable-consistent-read = false
# 是否启用导入功能
# 启用 csv 数据导入功能
# 1，需要将 dbms-worker 与下游数据库实例部署在同台服务器，并且对应的数据库用户需要具备对应的数据库权限
# 2，数据导入以表串行机制运行
# 3，csv 数据文件字符集、字段分隔符、字段定定界符号、NULL 值映射、行分隔符无需单独配置，自动继承 csv-migrate-params 值
# 4，csv 数据导入所在目标表必须是空表，若非空表，导入数据报错
# 5，csv 数据数据导入功能属于有状态任务，enable-import-feature 参数一旦创建，则该参数的修改无法修改，即无法从 false -> true，true -> false，除非是新创建任务重新开始
enable-import-feature = true
# 参数以 Key-Value 形式配置（具体以下游数据库对应参数配置为准）
csv-import-params = {"disableTikvImportMode"="false", "Thread"="10"}